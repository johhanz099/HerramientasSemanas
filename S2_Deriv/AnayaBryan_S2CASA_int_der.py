# -*- coding: utf-8 -*-
"""AnayaBryan_S2CASA_int_der.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jYHGXT1IelbPpmJuQQnwONdlvc8lw5hL
"""

import numpy as np
import matplotlib.pylab as plt

## Integración
# Se define la función coseno
def funcion(x):
    r = np.cos(x)
    return r

# El intervalo a integrar la funcion es [0:3pi/2]
x = np.linspace(0,3*np.pi/2,10)
y = funcion(x)

# Define el método para imprimir la comparacion entre valor teorico y el obtenido
def imp_comp(name,val_t,r):
    print("Usando el método de integración",name,"se obtuvo",r,'\n',"Comparado con el valor analítico de",val_t,'\n')

# Definir métodos de integración

# Suma de rectángulos
def f_sum_rect(x,y):
    base = x[1] - x[0]
    alt  = y[1:]
    area = alt*base
    r = np.sum(area)
    return r

# Prueba suma de rectángulos
#print(f_sum_rect(x,y))

# Trapezoide
def f_trapez(x,y):
    h = x[1]-x[0]
    p13 = 0.5*y[0] + 0.5*y[-1]
    p2 = np.sum(y[1:-1])
    r = h*(p13+p2)
    return r

# Prueba Trapezoide
#print(f_trapez(x,y))

# Simpson
def f_simps(x,y):
    h = x[1]-x[0]
    i=1
    y_s = 0
    while i<len(x):
        if i%2 == 0:
            y_par = 2*y[i]
            y_s = y_s + y_par
            i = i+1
        else:
            y_imp = 4*y[i]
            y_s = y_s + y_imp
            i = i+1
    suma = y_s+y[0]+y[-1]
    return (h/3)*suma   

# Prueba Simpson
print(f_simps(x,y))

# Comparar valores analíticos y obtenidos
sum_rec= f_sum_rect(x,y)
trapez = f_trapez(x,y)
simps = f_simps(x,y)

imp_comp("Suma de rectángulos",-1,round(sum_rec,3))
imp_comp("Trapezoide",-1,round(trapez,3))
imp_comp("Simpson",-1,simps)

## Derivación
# Usando la misma función <<funcion(x)>>
# Intervalo [0,2pi]
x_d = np.linspace(0,2*np.pi,30)
y_d = funcion(x_d)

# Forward difference
def forward_difference(x,y):
    yt = y[:-1]
    yf = y[1:]
    df = x[1]-x[0]
    sup = (yf-yt)/df
    return sup

# Central difference
def central_difference(x,y):
    y_bef = y[:-2]
    y_cen = y[1:-1]
    y_aft = y[2:]
    df = (x[2]-x[0])
    sup = (y_aft - y_bef)/df
    return sup

# Graficar y_d, y_dp (derivada primera) usando los dos métdos
y_dp_fd = forward_difference(x_d,y_d)
y_dp_cd = central_difference(x_d,y_d)

fig1_der = plt.figure(figsize=(15,8))
ax1 = fig1_der.add_subplot(111)
#ax2 = fig_der1.add_subplot(122)

ax1.plot(x_d[1:],y_dp_fd,label='$y\'$ vs $x$ Forward diff')
ax1.plot(x_d[1:-1],y_dp_cd,'r',label='$y\'$ vs $x$ Central diff')
ax1.set_title('$y\'$'+' Usando los dos métodos')
ax1.set_xlabel('$x$')
ax1.set_ylabel('$y\'$')
ax1.legend()
ax1.grid()

#ax2.plot(x_d[1:-1],y_dp_cd,label='$y\'$ vs $x$')
#ax2.set_title('$y\'$'+' Central difference')
#ax2.set_xlabel('$x$')
#ax2.set_ylabel('$y\'$')
#ax2.legend()
#ax2.grid()

plt.savefig('DerivadaFun.png')

# Graficar comparando con el valor analítico/teórico
# En este caso y_dpt = -sin(x)
y_dpt = -np.sin(x_d)
err_fd = np.abs(y_dpt[1:] - y_dp_fd)
err_cd = np.abs(y_dpt[1:-1] - y_dp_cd)

fig2_der = plt.figure(figsize=(15,8))
ax1 = fig2_der.add_subplot(121)
ax2 = fig2_der.add_subplot(122)

ax1.plot(x_d[1:],err_fd,label='$|v_{teo} - v_{num}|$')
ax1.set_title('Error forward difference')
ax1.set_xlabel('$x$')
ax1.set_ylabel('$|Valor_{teo} - Valor_{num}|$')
ax1.legend()
ax1.grid()

ax2.plot(x_d[1:-1],err_cd,'r',label='$|v_{teo} - v_{num}|$')
ax2.set_title('Error central difference')
ax2.set_xlabel('$x$')
ax2.set_ylabel('$|Valor_{teo} - Valor_{num}|$')
ax2.legend()
ax2.grid()

plt.savefig('ErrorDerivada.png')

# Implementar un algoritmo para la segunda derivada
def seg_der(x,y):
    y_bef = y[:-2]
    y_t = 2*y[1:-1]
    y_aft = y[2:]
    df = (x[1]-x[0])
    sup = (y_aft + y_bef - y_t)/df**2
    return sup

# Graficar segunda derivada
# y_dst = -cos(x)
y_ds = seg_der(x_d,y_d)
y_dst = -np.cos(x_d)

fig3_der = plt.figure(figsize=(15,8))
ax1 = fig3_der.add_subplot(111)

ax1.plot(x_d,y_dst,label='$y\'\'_{teo} = -\cos{x}$')
ax1.plot(x_d[1:-1],y_ds,'r',label='$y\'\'_{num}$')
ax1.set_title('Segunda derivada comparada con el valor analítico')
ax1.set_xlabel('$x$')
ax1.set_ylabel('$y\'\'$')
ax1.legend()
ax1.grid()

plt.savefig('2DerivadaFun.pdf')

## Raíces
# Definir el polinomio
def poli(x):
    return (x**5)-(2.0*x**4)-(10*x**3)+(20*x**2)+(9*x)-18

def poli_d(x):
    return (5*x**4)-(8*x**3)-(30*x**2)+(40*x)+9

x_p = np.linspace(-4,4,30)

fig1_rt = plt.figure(figsize=(15,8))
ax1 = fig1_rt.add_subplot(111)

ax1.plot(x_p,poli(x_p),label='$f(x) = x^5-2x^4-10x^3+20x^2+9x-18$')
ax1.set_title('Polinomio')
ax1.set_xlabel('$x$')
ax1.set_ylabel('$f(x)$')
ax1.legend()
ax1.grid()

plt.savefig('AnayaBryan_NRpoli.pdf')

# Definir método Newton-Rhapson
def NewtonR(pol,pol_d,x_g,iter):
    i=0
    while i <= iter:
        x_g = x_g - pol(x_g)/pol_d(x_g)
        i = i+1
    return x_g

x_0r = NewtonR(poli,poli_d,-3,3)
print("Usando el método de Newton-Rhapson un x_guess = -3, y con 3 iteraciones se obtiene un cero en")
print("x_0r =",x_0r)
print("Además, el valor del polinomio evaluado en x_0r es:",poli(x_0r))







